{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP autoencoders\n",
    "\n",
    "An autoencoder is a network which consists of two main parts:\n",
    "- an *encoder*, which projects the data into a latent space to transform it into a compact representation.\n",
    "- a *decoder*, which reconstructs the input data from the latent representation.\n",
    "\n",
    "Autoencoders are very useful in many applications. For instance, in image processing, they are used for image denoising, compression, and generative models (image synthesis and transformation). They can also be used for transfer learning: first an autoencoder is trained to learn a latent representation of the data, and then this representation can be used for other classification/regression tasks.\n",
    "\n",
    "<center><a href=\"https://emkademy.medium.com/1-first-step-to-generative-deep-learning-with-autoencoders-22bd41e56d18\">\n",
    "    <img src=\"https://miro.medium.com/max/772/1*ztZn098tDQsnD5J6v1eNuQ.png\" width=\"600\"></a></center>\n",
    "\n",
    "**Note**: We will study an application of autoencoders to image denoising in lab 6, but here we study a simple autoencoder which compresses and reconstruct an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/mnt/c/Master2/DeepLearning/lab4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data repository\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Load the MNIST dataset\n",
    "data_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_data = torchvision.datasets.MNIST(data_dir, train=True, download=True, transform=data_transforms)\n",
    "test_data = torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=data_transforms)\n",
    "num_classes = len(train_data.classes)\n",
    "\n",
    "# Take a subset of the train/test data\n",
    "train_data = Subset(train_data, torch.arange(400))\n",
    "test_data = Subset(test_data, torch.arange(50))\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images and corresponding labels from the train dataloader\n",
    "image_batch_example = next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The encoder\n",
    "\n",
    "First, let us write the encoder. We consider a 3-layer encoder, where each layer consists of a Linear part and a ReLU. The first layer goes from size `input_size` to 128, the second layer from 128 to 64, and the third layer from 64 to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the encoder class ('__init__' and 'forward' methods)\n",
    "\n",
    "class MLPencoder(nn.Module):\n",
    "    def __init__(self, input_size): \n",
    "        super(MLPencoder, self).__init__()\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, 128), nn.ReLU())\n",
    "        self.hidden1 = nn.Sequential(nn.Linear(128, 64), nn.ReLU())\n",
    "        self.hidden2 = nn.Sequential(nn.Linear(64, 32), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden1(x)\n",
    "        z = self.hidden2(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "# TO DO:\n",
    "# - instanciate an encoder (get the proper input size)\n",
    "# - vectorize image_batch_example into image_batch_example_vec\n",
    "# - apply the encoder to image_batch_example_vec to produce the latent representation z\n",
    "# - print the size of z\n",
    "input_size = train_data[0][0][0].shape[0]*train_data[0][0][0].shape[1]\n",
    "model_example = MLPencoder(input_size)\n",
    "image_batch_example_vec = image_batch_example.view(-1, 784)\n",
    "z = model_example(image_batch_example_vec)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The decoder\n",
    "\n",
    "The decoder as a similar structure than the encoder (3 {Linear + ReLU} layers) but the sizes are flipped: the first layer goes from 32 to 64, then 64 to 128, and then 128 back to the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the decoder class ('__init__' and 'forward' methods)\n",
    "\n",
    "class MLPdecoder(nn.Module):\n",
    "    def __init__(self, input_size): \n",
    "        super(MLPdecoder, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(32, 64), nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Linear(64,128), nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(nn.Linear(128, input_size), nn.ReLU())\n",
    "    def forward(self, z):\n",
    "        z = self.layer1(z)\n",
    "        z = self.layer2(z)\n",
    "        y = self.layer3(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 784])\n"
     ]
    }
   ],
   "source": [
    "# TO DO:\n",
    "# - instanciate a decoder\n",
    "# - apply it to the latent representation z computed before\n",
    "# - print the size of the output y (should be the same as the encoder input 'image_batch_example_vec')\n",
    "decoder_model = MLPdecoder(input_size)\n",
    "y = decoder_model(z)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The autoencoder main module\n",
    "\n",
    "Finally we can write the autoencoder module, which consists of the encoder and the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the MLP autoencoder class using the previously written encoder and decoder\n",
    "class MLPAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size): \n",
    "        super(MLPAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = MLPencoder(input_size)\n",
    "        self.decoder = MLPdecoder(input_size)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Instanciate an MLP autoencoder and print the number of parameters.\n",
    "ex = MLPAutoencoder(784)(image_batch_example_vec)\n",
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPAutoencoder(input_size=784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization (ensure reproducibility: everybody should have the same results)\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q3**</span> How many parameters are in the autoencoder?\n",
    "\n",
    "## Training\n",
    "\n",
    "We can now write the training function. It's very similar to the function for training the MLP classifier (we don't use early stopping here). There are two main differences:\n",
    "- since we don't try to predict a label, we don't need to load them when iterating over the dataloader.\n",
    "- the loss function is no longer Cross Entropy (which is for classification), but MSE (*cf.* lab 2).\n",
    "- as optimizer we no longer use the `SGD`, but `Adam`, which is a better algorithm (but uses the same parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the training function (you can reuse the code from lab 3 or 4.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - Train the autoencoder: 300 epochs, learning rate = 0.001, and MSE loss function\n",
    "# - After training, save the model parameters and plot the loss over epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the autoencoder to the image_batch_example\n",
    "predicted_batch_example = model(image_batch_example_vec).detach()\n",
    "\n",
    "# Reshape it as a black-and-white image (3D tensor)\n",
    "predicted_batch_example = predicted_batch_example.reshape(batch_size, 1, 28, 28)\n",
    "\n",
    "# Plot the original and predicted images\n",
    "for ib in range(batch_size):\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_batch_example[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Original image')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(predicted_batch_example[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Predicted image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q4**</span> Put one of the plots above in your report (no need to put all of them, we just need one original and the corresponding predicted image).\n",
    "\n",
    "## Validation loss and early stopping (bonus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the train and validation sets and dataloaders as in the previous script\n",
    "n_train_examples = int(len(train_data)*0.9)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = random_split(train_data, [n_train_examples, n_valid_examples])\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Write the evaluation function.\n",
    "# It's similar as in the previous script, but we compute the MSE loss, not accuracy (because it's not classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the training function with validation (you can reuse the code from lab 3 and/or 4.1)\n",
    "# Be careful: since we compute the validation loss (not accuracy), it should deacrease, not increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with validation\n",
    "model = MLPAutoencoder(input_size)\n",
    "torch.manual_seed(0)\n",
    "model.apply(init_weights)\n",
    "model, loss_all_epochs, val_loss_all_epochs = train_val_mlp_autoencoder(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses over epochs\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_all_epochs)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_loss_all_epochs)\n",
    "plt.title('Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q5**</span> Put the plot above in your report (training and validation losses over epochs). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
