{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data processing with recurrent neural networks\n",
    "\n",
    "In this series of labs (7, 8, and 9), we use recurrent neural networks (RNNs) to manipulate text data. RNNs are appropriate to handle sequential data where each point depends on the previous ones. This is the case, e.g., for time series (audio/speech signals, financial data...), but also videos (time sequence of images), or text.\n",
    "\n",
    "We will consider the problem of sequence to sequence (seq2seq) learning using RNNs. This task consists in producing one sequence of data from another, of possibly different lengths (it is one example of *many-to-many* RNNs you have studied during lectures).\n",
    "\n",
    "More specifically, we will work with textual data for the *machine translation* task: the goal is to automatically translate a sentence from one language to another.\n",
    "\n",
    "<center><a href=\"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\">\n",
    "    <img src=\"https://pytorch.org/tutorials/_images/seq2seq.png\" width=\"500\"></a></center>\n",
    "\n",
    "In this lab, we study the global pipeline for preprocessing text data, the basics of RNNs, and we write the encoder for the seq2seq model.\n",
    "\n",
    "**Note**: This notebook is based on [this tutorial](https://github.com/bentrevett/pytorch-seq2seq), which you are strongly encouraged to check as it goes into much more details about seq2seq models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: jinja2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.23.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: setuptools in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.28.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from de-core-news-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.2)\n",
      "Requirement already satisfied: setuptools in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (57.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.10)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.23.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.64.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# We'll be using torchtext and spacy to do most of the pre-processing\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# Download specific language models (comment if already done)\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!python3 -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text data\n",
    "\n",
    "### Tokenizers\n",
    "\n",
    "The first step is to define *tokenizers*, that is, how a string is transformed into a sequence of words (or *tokens*). For instance, \"Welcome to the U.S.A.!\" should be transformed into \\[\"Welcome\", \"to\", \"the\", \"U.S.A.\", \"!\"\\]. These are called tokens in the general case because \"!\" is not a word.\n",
    "\n",
    "We will also use language models, in order to preserve some specific rules for tokenization: with a naïve approach, \"U.S.A.\" would be split into 6 tokens \\[\"U\", \".\", \"S', \".\", \"A\", \".\"\\], but we want to consider it as a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the U.S.A!\n",
      "['Welcome', 'to', 'the', 'U.S.A.', '!']\n"
     ]
    }
   ],
   "source": [
    "# load the German and English specific pipelines\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# define tokenizers\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# check on an example\n",
    "print('Welcome to the U.S.A!')\n",
    "print(tokenize_en('Welcome to the U.S.A.!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also define 'Fields', which handle how the data will be processed. In addition to tokenizing, it can convert\n",
    "# all characters to lower case and add extra tokens for the start and end of sentences.\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We use the Multi30k dataset, which contains sentences in German and English (as well as French, but it's not available using torchtext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 3.46MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 639kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 747kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n",
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "train_data, valid_data, test_data = Multi30k.splits(root='data/', exts = ('.de', '.en'), fields = (SRC, TRG))\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "# We take a subset of the full dataset for speed\n",
    "train_data.examples = train_data.examples[:1000]\n",
    "valid_data.examples = valid_data.examples[:100]\n",
    "test_data.examples = train_data.examples[:100]\n",
    "\n",
    "# Print one example\n",
    "print(train_data.examples[0].src)\n",
    "print(train_data.examples[0].trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "We now use the ```build_vocab``` method of the ```Field``` to create a vocabulary from the data. A vocabulary maps each token to an integer, and using the Field also transforms it into a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 802\n",
      "Unique tokens in target (en) vocabulary: 823\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<pad>\n",
      "<sos>\n",
      "<eos>\n",
      "a\n",
      ".\n",
      "in\n",
      "the\n",
      "on\n",
      "man\n",
      "and\n",
      "is\n",
      "of\n",
      "with\n",
      "are\n",
      "two\n",
      ",\n",
      "woman\n",
      "at\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "# We can use the 'vocab.itos' method to convert indices to the corresponding tokens\n",
    "# The first tokens in the vocabulary are 'unknown word', 'padding', 'start of sentence' and 'end of sequence'.\n",
    "# Then the tokens are ranked by frequency of appearence in the dataset.\n",
    "for i in range(20):\n",
    "    print(TRG.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "The final step is to create a dataloader to generate batches of data. Instead of using the classic ```Dataloader``` function, we use ```BucketIterator```, which creates batches by assembling sentences of same or close lengths. This reduces the amount of padding and therefore of useless calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  2,   5,  13,  25,  10,   7, 474,   4,   3,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1])\n",
      "torch.Size([33, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_dataloader, _, test_dataloader = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                           batch_size = batch_size)\n",
    "\n",
    "# Fetch one batch as an example\n",
    "example_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Each batch contains a 'src' and 'trg' entries (source and target), corresponding to English and German sentences.\n",
    "print(example_batch.src[:,1])\n",
    "\n",
    "# The shape of this tensor should be [seq_length, batch_size] where seq_length is the maximum length of a sentence in this batch\n",
    "print(example_batch.src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write a function that takes a list of integers (such as a slice of 'example_batch' above) as input\n",
    "# and return the corresponding tokens (hint: use the 'vocab.itos' method)\n",
    "# only keep the token after '<sos>' and before '<eos>'\n",
    "def indx2tokens_list(token_list, token_dic):\n",
    "    return [token_dic.vocab.itos[i] for i in token_list]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'ein', 'mann', 'sitzt', 'auf', 'einem', 'stein', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<sos>', 'a', 'man', 'sits', 'on', 'a', 'rock', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Apply this function to a source sentence and its corresponding target translation\n",
    "indx_example = 1\n",
    "print(indx2tokens_list(example_batch.src[:, indx_example], SRC))\n",
    "print(indx2tokens_list(example_batch.trg[:, indx_example], TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q1**</span> Put these sentences in your report.\n",
    "\n",
    "## Recurrent networks basics\n",
    "\n",
    "Now that the data is ready, let's see the basic operations used in RNNs: [embedding layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding), [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html), and [recurrent layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers).\n",
    "\n",
    "### Embedding layer\n",
    "\n",
    "Sentences have been tokenized and tokens have been transformed into integers. We need to further transform these integers into word vectors: the idea is that two *similar* words should have similar word vectors. \n",
    "\n",
    "\n",
    "<center><a href=\"https://ruder.io/word-embeddings-1/\">\n",
    "    <img src=\"https://ruder.io/content/images/size/w2000/2016/04/word_embeddings_colah.png\" width=\"500\"></a></center>\n",
    "\n",
    "This notion of similarity (and what word vectors exactly represent) is hard to define explicitly. Then, we use an embedding layer to produce these word vectors, and this layer is learned during training.\n",
    "Many pre-trained word embeddings are available (e.g., word2vec) but here we will learn it from scratch along with the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.9498, -0.2377,  0.3892,  ...,  0.6667, -1.0490,  1.5987],\n",
      "         [ 1.9498, -0.2377,  0.3892,  ...,  0.6667, -1.0490,  1.5987],\n",
      "         [ 1.9498, -0.2377,  0.3892,  ...,  0.6667, -1.0490,  1.5987],\n",
      "         ...,\n",
      "         [ 1.9498, -0.2377,  0.3892,  ...,  0.6667, -1.0490,  1.5987],\n",
      "         [ 1.9498, -0.2377,  0.3892,  ...,  0.6667, -1.0490,  1.5987],\n",
      "         [ 1.9498, -0.2377,  0.3892,  ...,  0.6667, -1.0490,  1.5987]],\n",
      "\n",
      "        [[ 0.5344,  0.5631, -0.9296,  ...,  1.5760,  0.9768, -0.9856],\n",
      "         [ 0.5344,  0.5631, -0.9296,  ...,  1.5760,  0.9768, -0.9856],\n",
      "         [ 0.5344,  0.5631, -0.9296,  ...,  1.5760,  0.9768, -0.9856],\n",
      "         ...,\n",
      "         [ 1.3062, -1.5726,  0.8415,  ...,  0.1753, -0.3047, -0.8642],\n",
      "         [ 0.5344,  0.5631, -0.9296,  ...,  1.5760,  0.9768, -0.9856],\n",
      "         [ 0.2472,  0.4570, -1.9588,  ...,  1.0134,  1.1166, -0.2600]],\n",
      "\n",
      "        [[-0.6605,  0.5439,  0.9969,  ...,  0.2231, -0.0096, -2.1519],\n",
      "         [-1.6911,  1.0794, -0.5341,  ..., -0.4609,  0.2001, -0.0099],\n",
      "         [-0.5556, -1.8464, -1.8852,  ..., -1.3083,  0.0790,  1.5302],\n",
      "         ...,\n",
      "         [-0.5556, -1.8464, -1.8852,  ..., -1.3083,  0.0790,  1.5302],\n",
      "         [-1.6911,  1.0794, -0.5341,  ..., -0.4609,  0.2001, -0.0099],\n",
      "         [ 1.5009, -0.5415,  0.0921,  ..., -1.0457, -0.2490, -0.0818]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         ...,\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657]],\n",
      "\n",
      "        [[ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         ...,\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657]],\n",
      "\n",
      "        [[ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         ...,\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657],\n",
      "         [ 0.8954,  0.9632,  1.6506,  ...,  0.4715, -1.0668,  1.3657]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([33, 128, 32])\n"
     ]
    }
   ],
   "source": [
    "# Create an embedding layer. We need to specify:\n",
    "# - the input size, that is, how many words are in the vocabulary\n",
    "# - the embedding size, that is, how \"big\" is the word vectors space \n",
    "input_size = len(SRC.vocab)\n",
    "emb_size = 32\n",
    "src_emb_layer = nn.Embedding(input_size, emb_size)\n",
    "\n",
    "# Apply it to the example batch and display it\n",
    "embedded_batch = src_emb_layer(example_batch.src)\n",
    "print(embedded_batch)\n",
    "\n",
    "# The size of the word vectors for a batch should be [seq_length, batch_size, emb_size]\n",
    "print(embedded_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "The core idea of a dropout layer is to reduce the risk of overfitting by randomly setting some inputed values at 0. Since the non-zero inputs (and the corresponding weights in the network) are not the same from one batch to another, it results in forcing these weights not to be batch-specific, and therefore avoid overfitting. Dropout can be used for any network (including MLP or CNNs), but it's more important for RNNs, which are more prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.8997, -0.0000,  0.0000,  ...,  1.3334, -2.0979,  3.1975],\n",
      "         [ 0.0000, -0.0000,  0.7785,  ...,  1.3334, -0.0000,  3.1975],\n",
      "         [ 0.0000, -0.0000,  0.7785,  ...,  1.3334, -2.0979,  3.1975],\n",
      "         ...,\n",
      "         [ 3.8997, -0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
      "         [ 3.8997, -0.0000,  0.7785,  ...,  0.0000, -2.0979,  0.0000],\n",
      "         [ 0.0000, -0.0000,  0.7785,  ...,  0.0000, -0.0000,  3.1975]],\n",
      "\n",
      "        [[ 1.0689,  0.0000, -1.8591,  ...,  0.0000,  1.9537, -0.0000],\n",
      "         [ 1.0689,  0.0000, -0.0000,  ...,  3.1520,  0.0000, -1.9712],\n",
      "         [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "         ...,\n",
      "         [ 2.6125, -0.0000,  0.0000,  ...,  0.3506, -0.0000, -1.7283],\n",
      "         [ 1.0689,  1.1262, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000, -3.9176,  ...,  0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[-1.3211,  0.0000,  0.0000,  ...,  0.4462, -0.0192, -0.0000],\n",
      "         [-3.3822,  2.1588, -0.0000,  ..., -0.0000,  0.0000, -0.0198],\n",
      "         [-1.1112, -3.6928, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000, -0.0000,  ..., -2.6166,  0.0000,  0.0000],\n",
      "         [-0.0000,  2.1588, -1.0682,  ..., -0.9219,  0.4001, -0.0198],\n",
      "         [ 3.0019, -0.0000,  0.0000,  ..., -2.0914, -0.4979, -0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.7908,  1.9264,  3.3011,  ...,  0.0000, -2.1337,  2.7313],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -2.1337,  2.7313],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.9431, -2.1337,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  1.9264,  3.3011,  ...,  0.9431, -2.1337,  0.0000],\n",
      "         [ 1.7908,  1.9264,  0.0000,  ...,  0.0000, -0.0000,  2.7313],\n",
      "         [ 1.7908,  1.9264,  0.0000,  ...,  0.0000, -2.1337,  2.7313]],\n",
      "\n",
      "        [[ 1.7908,  0.0000,  0.0000,  ...,  0.0000, -2.1337,  2.7313],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.9431, -0.0000,  0.0000],\n",
      "         [ 1.7908,  0.0000,  0.0000,  ...,  0.0000, -2.1337,  2.7313],\n",
      "         ...,\n",
      "         [ 0.0000,  1.9264,  0.0000,  ...,  0.9431, -0.0000,  0.0000],\n",
      "         [ 1.7908,  1.9264,  0.0000,  ...,  0.9431, -0.0000,  2.7313],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.9431, -0.0000,  2.7313]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  3.3011,  ...,  0.9431, -0.0000,  2.7313],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -2.1337,  0.0000],\n",
      "         [ 1.7908,  1.9264,  3.3011,  ...,  0.9431, -2.1337,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  1.9264,  3.3011,  ...,  0.9431, -2.1337,  2.7313],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.9431, -0.0000,  2.7313],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -2.1337,  2.7313]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# the percentage of zeroed values (expressed between 0 and 1) is given as input\n",
    "dropout_layer = nn.Dropout(0.5)\n",
    "drop_batch = dropout_layer(embedded_batch)\n",
    "\n",
    "# in this example, half of the entries (50%) are set at 0\n",
    "print(drop_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent layers\n",
    "\n",
    "<center><a href=\"https://www.researchgate.net/profile/Rezzy-Caraka/publication/346410173_Employing_Long_Short-Term_Memory_and_Facebook_Prophet_Model_in_Air_Temperature_Forecasting/links/60077104a6fdccdcb868957f/Employing-Long-Short-Term-Memory-and-Facebook-Prophet-Model-in-Air-Temperature-Forecasting.pdf\">\n",
    "    <img src=\"https://www.researchgate.net/profile/Rezzy-Caraka/publication/346410173/figure/fig2/AS:962598073823234@1606512673418/Network-Structure-of-RNN-LSTM-and-GRU.png\" width=\"500\"></a></center>\n",
    "\n",
    "We now see the 3 main recurrent layers (simple RNN, LSTM and GRU). We won't focus on the technical difference between these, but you can find more info online (e.g., [here](https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573)).\n",
    "\n",
    "#### Simple RNN\n",
    "\n",
    "First, let's see the basic RNN. We note $x_t$ the $t$-th element of the input to the RNN (in our case: this is the embedding after dropout). We have $h_t = \\text{RNN}(x_t, h_{t-1})$, where $h_{t}$ is the hidden state. To define such an RNN in Pytorch (using ```nn.RNN```), you need to specify:\n",
    "\n",
    "- the size of the input (here, it's the size of the embeddings)\n",
    "- the size of the hidden space (`hidden_size`)\n",
    "- the number of layers (`n_layers`)\n",
    "\n",
    "By default, the RNN is uni-directional, uses bias, and uses tanh as activation function. If you use a multi-layer RNN, you can also add dropout in the intermediate layers. You can change these by playing with the parameters of the function (see the [doc](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN) for more info).\n",
    "\n",
    "**Note**: for the first element of the sequence, we have $h_1 = \\text{RNN}(x_1, h_{0})$, so we normally need to provide an initial hidden state $h_0$. In pytorch, we can either provide it explicitly or not. If we don't, it will use $h_0=0$ by default. This is what we do here, and it also applies to other recurrent units (LSTM and GRU).\n",
    "\n",
    "In Pytorch, applying an RNN returns not one, but two outputs, usually called `out` and `hidden`, illustrated below:\n",
    "\n",
    "- `out` is the whole sequence of hidden states of the last layer\n",
    "- `hidden` is the hidden state of the last token for all layers\n",
    "\n",
    "<center><img src=\"rnn_outputs.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 128, 50])\n",
      "torch.Size([2, 128, 50])\n"
     ]
    }
   ],
   "source": [
    "# Define a basic RNN\n",
    "hidden_size = 50\n",
    "n_layers = 2\n",
    "rnn = nn.RNN(emb_size, hidden_size, n_layers)\n",
    "\n",
    "# Apply the RNN to the input (embeddings after dropout, called 'drop_batch')\n",
    "rnn_out, rnn_hidden = rnn(drop_batch)\n",
    "\n",
    "# Get the size of the 'rnn_out': it should be [seq_length, batch_size, hidden_size]\n",
    "print(rnn_out.shape)\n",
    "\n",
    "# Get the size of the'rnn_hidden': it should be [n_layers, batch_size, hidden_size]\n",
    "print(rnn_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - create a 3-layer bidirectional RNN (check the doc!)\n",
    "# - apply it to the same input 'embedded_batch'\n",
    "\n",
    "# Print the size of rnn_out (it should be [seq_length, batch_size, 2*hidden_size])\n",
    "# Print the size of the final hidden state (it should be [2*n_layers, batch_size, hidden_size])\n",
    "# (the factor '2' in the shapes comes from the fact that the network is bidirectional)\n",
    "rnn_2 = nn.Sequential( \n",
    "                      nn.RNN(emb_size, hidden_size, 3, bidirectional = True))\n",
    "\n",
    "rnn_out, rnn_hidden = rnn_2(embedded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 128, 100])\n",
      "torch.Size([6, 128, 50])\n"
     ]
    }
   ],
   "source": [
    "print(rnn_out.shape) \n",
    "print(rnn_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q2**</span> Put the sizes of these outputs in your report.\n",
    "\n",
    "#### LSTM\n",
    "\n",
    "The basic RNN suffers from the gradient vanishing problem, so we will instead use a variant of it called *long short-term memory* (LSTM) networks. The key idea of LSTM is that it has an extra hidden feature called a *cell state* which allows the network to \"remember\" which part of the input sequence is useful or not, and therefore to avoid backpropagating the gradient throughout the whole sequence, thus avoiding gradient vanishing.\n",
    "\n",
    "The formula for the [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is therefore: $(h_t, c_t) = \\text{LSTM}(x_t, h_{t-1}, c_{t-1})$ where $c_t$ is this extra cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 128, 50])\n",
      "torch.Size([2, 128, 50])\n",
      "torch.Size([2, 128, 50])\n"
     ]
    }
   ],
   "source": [
    "# Define an LSTM\n",
    "hidden_size = 50\n",
    "n_layers = 2\n",
    "lstm = nn.LSTM(emb_size, hidden_size, n_layers)\n",
    "\n",
    "# Apply the LSTM to the embedded batch\n",
    "lstm_out, (lstm_hidden, lstm_cell) = lstm(embedded_batch)\n",
    "\n",
    "# The shape of the output and final hidden state are the same as before.\n",
    "# The final cell state as the same size as the final hidden state.\n",
    "print(lstm_out.shape)\n",
    "print(lstm_hidden.shape)\n",
    "print(lstm_cell.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU\n",
    "\n",
    "The last main type of recurrent layer is the gated reccurent unit (GRU). It is sort of a simplified LSTM: it also has some memory mechanism to avoid gradient vanishing, but it outputs only a single hidden state vector (instead of the additional cell state in LSTM). It generally performs similarly with LSTM (but this depends on applications). Writting a [GRU in pytorch](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU) is similar to a basic RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 128, 50])\n",
      "torch.Size([2, 128, 50])\n"
     ]
    }
   ],
   "source": [
    "# TO DO: using the doc, write a GRU layer with a hidden size of 50 and 2 layers.\n",
    "# Apply it to the embedded batch as before, and print the size of the output and final hidden state.\n",
    "gru = nn.GRU(emb_size, hidden_size, n_layers)\n",
    "gru_out, gru_hidden = lstm(embedded_batch)\n",
    "print(gru_out.shape)\n",
    "print(gru_hidden[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that all recurrent layers might also use dropout to randomly remove some of the inner RNN weights. This only makes sense if `n_layers` > 1, otherwise you'll get a warning.\n",
    "\n",
    "## Building the translation model\n",
    "\n",
    "We'll now build the machine translation model. This model is based on two part:\n",
    "\n",
    "- an *encoder*, which takes as input the source sentence (in German) and encodes it into a *context* vector. This context vector is sort of a summary of the whole input sentence.\n",
    "- a *decoder*, which takes as input this context vector and sequentially generates a sentence in English. It always starts with the `<sos>` token and uses the context vector to generate the second token. Then, it recursively uses the last produced token and the hidden state to generate the next token.\n",
    "\n",
    "<center><a href=\"https://github.com/bentrevett/pytorch-seq2seq\">\n",
    "    <img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/seq2seq1.png\"></a></center>\n",
    "\n",
    "On this picture, $h_t$ represent the hidden states of the encoder and $s_t$ the hidden states of the decoder. For LSTMs, we also need to consider the cell states, but they're not displayed here for brevity. The yellow blocks represent the embedding and dropout, the purple blocks represent the linear classifier, and the green blocks are the recurrent units.\n",
    "\n",
    "In this lab, we only write the encoder. It consists of:\n",
    "\n",
    "- an embedding layer to transform token indices into word vectors.\n",
    "- a dropout layer.\n",
    "- a single-layer LSTM, to learn the context vector.\n",
    "\n",
    "**Note**: We don't need to keep track of all the hidden states ($h_1$, $h_2$, $h_3$, etc.), we only need the final hidden state called the context vector (and denoted $z$ on the picture). Therefore, we can simply apply our LSTM on the whole sequence, instead of writting a loop explicitly (this will not be the case for the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO: initialize the network (remember to use 'self.' for all attributes / parameters / layers)\n",
    "        # - store the input parameters as class attributes\n",
    "        # - create the embedding layer (transform indices into word vectors)\n",
    "        # - create the dropout layer\n",
    "        # - create the LSTM layer\n",
    "        self.embed_layer = nn.Embedding(input_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        # TO DO: write the forward pass\n",
    "        # - compute the embeddings\n",
    "        # - apply dropout to the word embeddings\n",
    "        # - apply the LSTM layer\n",
    "        # - return the final hidden and cell states\n",
    "        embedded = self.dropout(self.embed_layer(src))\n",
    "        lstm_out, (lstm_hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        return lstm_hidden, lstm_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder parameters\n",
    "input_size = len(SRC.vocab)\n",
    "emb_size_enc = 32\n",
    "hidden_size = 50\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 42464\n"
     ]
    }
   ],
   "source": [
    "# TO DO:\n",
    "# - Instanciate the encoder\n",
    "# - print the number of trainable parameters\n",
    "# - pass the example_batch to the encoder, and print the size of the two outputs\n",
    "encoder = LSTMEncoder(input_size, emb_size, hidden_size, n_layers, dropout_rate)\n",
    "print('Total number of parameters:', sum(p.numel() for p in encoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 50])\n",
      "torch.Size([33, 128, 50])\n"
     ]
    }
   ],
   "source": [
    "out, hidden =encoder(example_batch.src)\n",
    "print(out.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q3**</span> Put the number of parameters in the LSTM encoder in your report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e4653ef3f08bbcc46c48a23d8290ce23ff8798f7d85b3da9ffbf3074ca96bd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
