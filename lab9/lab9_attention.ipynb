{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent networks with Attention\n",
    "\n",
    "In labs 7 and 8, we have implemented a seq2seq model based on the encoder-decoder structure, which operates as follows:\n",
    "\n",
    "- The input sentence (source) is passed as input to the encoder, which compresses the information it contains into a *context vector*, which is the final output of the encoder.\n",
    "- This context vector is then passed to the decoder, which sequentially decodes and updates it (through its hidden states) and produces the translated output sentence (target).\n",
    "\n",
    "However, this might be limitting for the decoding part: indeed, at a given step of the decoding process, it might be preferrable to have access to *all* the hidden states from the encoder rather than a single hidden state. This would allow to know which parts of the input sentence are the most relevent to generate the current word in the output sentence.\n",
    "\n",
    "This can be implemented using a mechanism called **attention**, which is the topic of this lab.\n",
    "\n",
    "Note: As in the previous labs, we'll talk about context and attention *vectors*. In practice, these are not vectors but tensors, since we operate on batches of data. Nevertheless we'll talk about *vectors* since it is consistent with the underlying theory.\n",
    "\n",
    "<center><a href=\"https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b\">\n",
    "    <img src=\"https://miro.medium.com/max/2000/1*FP3zFjdFhNUWEJ9hxeIYOA.png\" width=\"800\"></a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import spacy\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "The pre-processing is the same as in lab 7 and 8, so we provide it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German and English specific pipelines\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenizers\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Fields\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "# Dataset\n",
    "train_data, valid_data, test_data = Multi30k.splits(root='data/', exts = ('.de', '.en'), fields = (SRC, TRG))\n",
    "\n",
    "# Take a subset of the dataset (for speed)\n",
    "train_data.examples = train_data.examples[:1000]\n",
    "valid_data.examples = valid_data.examples[:100]\n",
    "test_data.examples = train_data.examples[:100]\n",
    "\n",
    "# Vocabulary\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# Dataloader (here we keep the validation dataloader)\n",
    "batch_size = 128\n",
    "train_dataloader, valid_dataloader, test_dataloader = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index to string functions\n",
    "def indx2tokens_list(list_indx, field):\n",
    "    list_tokens = [field.vocab.itos[list_indx[i]] for i in range(len(list_indx))]\n",
    "    beg_i = list_tokens.index('<sos>')\n",
    "    if '<eos>' in list_tokens:\n",
    "        end_i = list_tokens.index('<eos>')\n",
    "    else:\n",
    "        end_i = -1\n",
    "    list_tokens = list_tokens[beg_i+1:end_i]\n",
    "    return list_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 128])\n"
     ]
    }
   ],
   "source": [
    "# Fetch one example\n",
    "example_batch = next(iter(train_dataloader))\n",
    "print(example_batch.src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the parameters of the network (small model for speed)\n",
    "input_size = len(SRC.vocab)\n",
    "output_size = len(TRG.vocab)\n",
    "emb_size_enc = 32\n",
    "emb_size_dec = 32\n",
    "hidden_size = 50\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU encoder\n",
    "\n",
    "We use a similar encoder to the previous lab. The main difference is that it needs to output the whole sequence of hidden states (not only the final hidden state / context vector) because we will use it in the attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size_enc, hidden_size, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO:\n",
    "        # - store the input parameters as class attributes\n",
    "        # - create the embedding, dropout, and GRU layers (the GRU layer does not use recurrent dropout)\n",
    "        self.input_size = input_size\n",
    "        self.emb_size_enc = emb_size_enc\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.embedding_layer = nn.Embedding(input_size, emb_size_enc)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.lstm = nn.GRU(emb_size_enc, hidden_size, n_layers)\n",
    "\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        # TO DO:\n",
    "        # - write the forward pass\n",
    "        # - return both outputs of the GRU (call them 'enc_outputs' and 'enc_context')\n",
    "        y = self.embedding_layer(src)\n",
    "        y = self.dropout(y)\n",
    "        enc_outputs, enc_context, = self.lstm(y)\n",
    "\n",
    "        return enc_outputs, enc_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 38264\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Instanciate the encoder and print the number of parameters\n",
    "\n",
    "# TO DO: Apply the encoder to the example batch and print the shapes of the outputs\n",
    "encoder_lstm = Encoder(input_size, emb_size_enc, hidden_size, n_layers, dropout_rate)\n",
    "enc_outputs, enc_context = encoder_lstm(example_batch.src)\n",
    "print('Number of parameters:', sum(p.numel() for p in encoder_lstm.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The attention mechanism\n",
    "\n",
    "We now implement the attention mechanism. Intuitively, the idea behind attention is to say which word of the source sentence is the most important to generate the current word at decoding.\n",
    "\n",
    "Mathematically, let's note $t'$ the current decoding step, $s_{t'-1}$ the previous hidden state of the decoder, and $H = \\{h_1, h_2,...,h_T \\}$  the set of all encoder outputs at the last layer. The attention vector $a_{t'}$ will therefore be calculated from $s_{t'-1}$ and $H$ through a set of operation with learnable parameters.\n",
    "\n",
    "### An example\n",
    "\n",
    "For the very first target token, the previous hidden state is given by $s_0$ = $z$ (= the context vector). The attention mechanism is illustrated below in this case, and we will consider it as example.\n",
    "\n",
    "<center><a href=\"https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\">\n",
    "    <img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/seq2seq9.png\" width=\"500\"></a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_len = enc_outputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 50])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the context vector of the encoder and treat it as the first hidden state to the decoder\n",
    "dec_hidden = enc_context\n",
    "dec_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to assemble / combine the current hidden state of the decoder $s_{t'-1}$ (here, it's simply the context vector) and the encoder outputs into a single tensor. There are many ways to do so (addition, multiplication...), here we simply concatenate them.\n",
    "\n",
    "**Note**: this is easy to implement when the recurrent part uses 1 layer; otherwise the computation would be more involved as we'd need to consider the extra dimension corresponding to the number of layers. We leave that to further exploration.\n",
    "\n",
    "Currently (on this example):\n",
    "- `enc_outputs` has a size of `[src_len, batch_size, hidden_size]`\n",
    "- `dec_hidden` has a size of `[1, batch_size, hidden_size]`\n",
    "\n",
    "We want to concatenate them along the last dimension, so first we need to repeat `dec_hidden` along the first dimension `src_len` times. Then, we can perform the concatenation. Finally, we permute the combined tensor so it has a size of `[batch_size, src_len, 2 * hidden_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 33, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: compute this concatenation:\n",
    "# - repeat it using the 'repeat' function (check the doc!)\n",
    "# - concatenate the features using the 'cat' function\n",
    "# - permute the dimensions so the resulting combined input has shape [batch_size, src_len, 2*hidden_size]\n",
    "dec_hidden = dec_hidden.repeat(enc_outputs.shape[0],1,1)\n",
    "concat_vec = torch.cat([dec_hidden, enc_outputs],dim=2).permute(1, 0, 2)\n",
    "concat_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write a network to compute attention from this combined tensor: $a = f(c)$. This network depends on two parameters values, which are:\n",
    " - `input_size_att`, which is the length of the input combined tensor, thus it's equal to `2 * hidden_size`.\n",
    " - `hidden_size_att`, which corresponds to the intermediate length used in computing the attention vector.\n",
    "\n",
    "The architecture of the network is as follows:\n",
    "\n",
    "- a linear layer that goes from size `input_size_att` to `hidden_size_att`\n",
    "- a tanh activation\n",
    "- a second linear layer, that goes from size `hidden_size_att` to `1`, and does not use bias\n",
    "- a softmax layer that acts along `dim=1`, to ensure that the attention vector sums to 1 for every sentence in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 33])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO:\n",
    "# - define the attention network parameters ('input_size_att', and 'hidden_size_att'=50)\n",
    "# - write the attention module\n",
    "# - apply it to the combined input tensor 'comb_inputs'\n",
    "# - squeeze the output and print its shape\n",
    "input_size_att = 2*hidden_size\n",
    "hidden_size_att = 50\n",
    "attent_net = nn.Sequential(nn.Linear(input_size_att, hidden_size_att),\n",
    "                            nn.Tanh(), nn.Linear(hidden_size_att, 1), nn.Softmax(dim=1))\n",
    "attention_output = attent_net(concat_vec)\n",
    "attention_output.squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attention module\n",
    "\n",
    "Now, we can use what we did above on the example to write the full attention module in the general case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size_att, hidden_size_att):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO: store the input parameters as attributes and define the network\n",
    "        self.input_size_att = input_size_att\n",
    "        self.hidden_size_att = hidden_size_att\n",
    "        self.attention = nn.Sequential(nn.Linear(input_size_att, hidden_size_att),\n",
    "                            nn.Tanh(), nn.Linear(hidden_size_att, 1), nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outputs):\n",
    "        \n",
    "        # TO DO: write the forward method to compute the attention vector\n",
    "        src_len = enc_outputs.shape[0]\n",
    "        dec_hidden = dec_hidden.repeat(src_len, 1, 1)\n",
    "        #print(enc_outputs.shape)\n",
    "        #print(dec_hidden.shape)\n",
    "        concat_vec = torch.cat([dec_hidden, enc_outputs], dim=2).permute(1, 0, 2)\n",
    "        attn = attent_net(concat_vec)\n",
    "        attn = attn.squeeze()\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - Instanciate an attention module\n",
    "# - Apply it using dec_hidden and enc_outputs\n",
    "# - Print the size of the output attention vector 'a'\n",
    "attention = Attention(input_size_att, hidden_size_att)\n",
    "a = attention(enc_context, enc_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 33])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q1**</span> Put the size of `a` in your report.\n",
    "\n",
    "## Decoder with attention\n",
    "\n",
    "At the decoding step, we can now use the attention vector by applying it to the encoder outputs. This results in the *weighted* vector which is the average of encoder outputs scaled by attention:\n",
    "\n",
    "$$\n",
    "w = \\sum_t a_t \\times h_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 50])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Compute the weighted vector.\n",
    "# - you need to expand/unsqueeze the attention vector 'a' so it has size [batch_size, 1, src_len]\n",
    "# - you also need to permute the enc_outputs vector so it has size [batch_size, src_len, hidden_size]\n",
    "# - perform multiplication using 'torch.bmm' (no need to use a for loop)\n",
    "# - permute the result so the weighted vector has size [1, batch_size, hidden_dim_dec]\n",
    "w = torch.bmm(a.unsqueeze(1), enc_outputs.permute(1, 0,2)).permute((1,0,2))\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to take this weighted vector into account when using the RNN (which here is a GRU). We remind that without attention, the recurrent computation is simply $s_{t'} = \\text{GRU}(y_{t'}, s_{t'-1})$ where $y_{t'}$ is the embedding after dropout. When using attention, the formula becomes:\n",
    "\n",
    "$$\n",
    "s_{t'} = \\text{GRU}([y_{t'}, w], s_{t'-1})\n",
    "$$\n",
    "\n",
    "This means the input of the GRU in the decoder is obtained by concatenating the weighted vector with the embedding after dropout $y_{t'}$.\n",
    "\n",
    "**Note**: using this concatenation changes the size of the GRU input: the input size should no longer be `emb_size_dec` but `emb_size_dec + hidden_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size_dec, hidden_size, hidden_size_att, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store input parameters as class attributes\n",
    "        self.output_size = output_size\n",
    "        self.emb_size_dec = emb_size_dec\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size_att = hidden_size_att\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "        self.embedding_layer = nn.Embedding(output_size, emb_size_dec)\n",
    "        # TO DO: Create the decoder layers and the attention module\n",
    "        self.gru = nn.GRU(emb_size_dec + hidden_size, hidden_size, n_layers)\n",
    "        self.attention = Attention(2*hidden_size, hidden_size_att)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_idx, input_hidden, enc_outputs):\n",
    "        \n",
    "        # Get the embeddings for the input token (same as in the previous lab)\n",
    "        y = self.dropout_layer(self.embedding_layer(input_idx))\n",
    "        y = y.unsqueeze(0)\n",
    "        \n",
    "        # TO DO:\n",
    "        # - compute the attention vector\n",
    "        # - compute the weighted vector\n",
    "        # - concatenate the embedding (after dropout) and the weighted vector\n",
    "        # - apply the GRU layer\n",
    "        # - squeeze the output of the GRU and pass it to the linear layer to have the predicted probabilites\n",
    "        # - return the predicted probabilites, the hidden state of the GRU, and the attention vector 'a'\n",
    "        a = attention(input_hidden, enc_outputs)\n",
    "        #print(a.shape)\n",
    "        w = torch.bmm(a.unsqueeze(1), enc_outputs.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        #print(w.shape)\n",
    "        concat_vec = torch.cat([y, w], dim=2)\n",
    "\n",
    "        #print(concat_vec.shape)\n",
    "        y, hidden_state = self.gru(concat_vec)\n",
    "        y = y.squeeze()\n",
    "        y = self.fc_out(y)\n",
    "        \n",
    "        return y, hidden_state, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the decoder: 93510\n",
      "torch.Size([1, 128, 50])\n",
      "torch.Size([1, 128, 50])\n",
      "torch.Size([128, 33])\n"
     ]
    }
   ],
   "source": [
    "# Instanciate the decoder, print the number of parameters\n",
    "decoder = Decoder(output_size, emb_size_dec, hidden_size, hidden_size_att, n_layers, dropout_rate)\n",
    "print('Number of parameters in the decoder:', sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "# Initialize an input index tensor (corresponds to <sos>)\n",
    "current_batch_size = a.shape[0]\n",
    "input_idx = torch.ones(current_batch_size).int() * 2\n",
    "\n",
    "# Apply the decoder, print the shape of the outputs\n",
    "pred_proba, hidden, a = decoder(input_idx, enc_context, enc_outputs)\n",
    "\n",
    "print(enc_context.shape)\n",
    "print(hidden.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q2**</span> How many parameters are in the decoder?\n",
    "\n",
    "## Full model\n",
    "\n",
    "The full model is the same as in the previous lab. The only difference comes from the fact that we store the attention vector at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store the encoder, decoder, and the target vocabulary size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.trg_vocab_size = decoder.output_size\n",
    "        \n",
    "    def forward(self, src, trg_len):\n",
    "        \n",
    "        # Create a tensor to store the predicted probabilities from the decoder\n",
    "        batch_size = src.shape[-1]\n",
    "        pred_probas_all = torch.zeros(trg_len, batch_size, self.trg_vocab_size)\n",
    "        \n",
    "        # Assign a probability of 1 to the token corresponding to <sos> for the first element\n",
    "        pred_probas_all[0, :, 2] = 1\n",
    "        \n",
    "        # Initialize the first input to the decoder as the <sos> token (coded by '2' in our vocabulary)\n",
    "        input_idx = torch.ones(batch_size).int() * 2\n",
    "        \n",
    "        # TO DO: apply the encoder to the src sentence and get the last hidden and cell states (=context vectors)\n",
    "        # (these will be used as initial hidden/cell for the decoder)\n",
    "        enc_outputs, enc_context = self.encoder(src)\n",
    "        # loop over tokens (note that it starts from 1 -not 0- since the very first token is already known (=<sos>))\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            # TO DO:\n",
    "            # - apply the decoder to get the predicted probabilites of the token t (and the updated hidden/cell)\n",
    "            # - store these predicted probabilities in the 'pred_probas_all' tensor\n",
    "            # - get the index corresponding to the highest probability for this token: it will be used as the next input index\n",
    "            output, hidden, a = self.decoder(input_idx, enc_context, enc_outputs)\n",
    "            input_idx = output.argmax(1)\n",
    "            pred_probas_all[t,:,:] = output\n",
    "        return pred_probas_all, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 128, 823])\n",
      "torch.Size([128, 33])\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed for reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Instanciate the full model, apply it to the example_batch, and print the output shapes\n",
    "encoder = Encoder(input_size, emb_size_enc, hidden_size, n_layers, dropout_rate)\n",
    "decoder = Decoder(output_size, emb_size_dec, hidden_size, hidden_size_att, n_layers, dropout_rate)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "trg_len = example_batch.trg.shape[0]\n",
    "pred_probas_all, attention_all = model(example_batch.src, trg_len)\n",
    "print(pred_probas_all.shape)\n",
    "print(attention_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (with validation) and evaluation\n",
    "\n",
    "We provide below the evaluation and training functions, since they're the same as in the previous lab (except we have to handle the fact that the model returns two outputs: `pred_probas_all` and `attention_all`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_seq2seq(model, eval_dataloader, loss_fn, SEED=1234):\n",
    "\n",
    "    # Set a random seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    # Set the model in 'eval' mode (disable dropout layer)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the eval loss\n",
    "    loss_eval = 0\n",
    "\n",
    "    # loop over batches\n",
    "    for i, batch in enumerate(eval_dataloader):\n",
    "\n",
    "        # Get the source and target sentence, and the target length, copy it to device\n",
    "        src, trg = batch.src, batch.trg\n",
    "        trg_len = trg.shape[0]\n",
    "\n",
    "        # Apply the model\n",
    "        pred_probas, _ = model(src, trg_len)\n",
    "\n",
    "        # Remove the first token (always <sos>) to compute the loss\n",
    "        output_size = pred_probas.shape[-1]\n",
    "        pred_probas = pred_probas[1:]\n",
    "\n",
    "        # Reshape the pred_probas and target so that they have appropriate sizes\n",
    "        pred_probas = pred_probas.view(-1, output_size)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(pred_probas, trg)\n",
    "\n",
    "        # Record the loss\n",
    "        loss_eval += loss.item()\n",
    "\n",
    "    return loss_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_val_seq2seq(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, verbose=True, SEED=1234):\n",
    "\n",
    "    # Set a random seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed)\n",
    "    model_tr.train()\n",
    "\n",
    "    # define the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize lists for storing the train and val losses\n",
    "    loss_train_total = []\n",
    "    loss_val_total = []\n",
    "    \n",
    "    loss_val_optim = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        loss_current_epoch = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Get the source and target sentence, and the target length, copy it to device\n",
    "            src, trg = batch.src, batch.trg\n",
    "            trg_len = trg.shape[0]\n",
    "\n",
    "            # Set the gradients at 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Apply the model\n",
    "            pred_probas, _ = model_tr(src, trg_len)\n",
    "\n",
    "            # Remove the first token (always <sos>) to compute the loss\n",
    "            output_dim = pred_probas.shape[-1]\n",
    "            pred_probas = pred_probas[1:]\n",
    "\n",
    "            # Reshape the pred_probas and target\n",
    "            pred_probas = pred_probas.view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss = loss_fn(pred_probas, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_tr.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss\n",
    "            loss_current_epoch += loss.item()\n",
    "\n",
    "        # At the end of each epoch, compute the validation loss\n",
    "        loss_val = evaluate_seq2seq(model_tr, valid_dataloader, loss_fn)\n",
    "        \n",
    "        # Record the training and validation losss over epochs\n",
    "        loss_train_total.append(loss_current_epoch)\n",
    "        loss_val_total.append(loss_val)\n",
    "        \n",
    "        # Display the training and validation losses\n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Training loss: {:.4f} ; Validation loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, loss_current_epoch, loss_val))\n",
    "            \n",
    "        # Save the current model as optimal only if validation loss decreases\n",
    "        if loss_val<loss_val_optim:\n",
    "            model_opt = copy.deepcopy(model_tr)\n",
    "            loss_val_optim = loss_val\n",
    "                \n",
    "    return model_opt, loss_train_total, loss_val_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Training loss: 53.5103 ; Validation loss: 6.6140\n",
      "Epoch [2/30], Training loss: 52.5802 ; Validation loss: 6.4413\n",
      "Epoch [3/30], Training loss: 50.7900 ; Validation loss: 6.0674\n",
      "Epoch [4/30], Training loss: 47.4231 ; Validation loss: 5.5361\n",
      "Epoch [5/30], Training loss: 43.4079 ; Validation loss: 5.0169\n",
      "Epoch [6/30], Training loss: 40.9285 ; Validation loss: 4.7114\n",
      "Epoch [7/30], Training loss: 38.6270 ; Validation loss: 4.5800\n",
      "Epoch [8/30], Training loss: 37.9342 ; Validation loss: 4.5011\n",
      "Epoch [9/30], Training loss: 37.6485 ; Validation loss: 4.4607\n",
      "Epoch [10/30], Training loss: 37.5373 ; Validation loss: 4.4410\n",
      "Epoch [11/30], Training loss: 37.4515 ; Validation loss: 4.4330\n",
      "Epoch [12/30], Training loss: 37.3679 ; Validation loss: 4.4296\n",
      "Epoch [13/30], Training loss: 37.2961 ; Validation loss: 4.4261\n",
      "Epoch [14/30], Training loss: 37.2637 ; Validation loss: 4.4226\n",
      "Epoch [15/30], Training loss: 37.2271 ; Validation loss: 4.4175\n",
      "Epoch [16/30], Training loss: 37.1891 ; Validation loss: 4.4139\n",
      "Epoch [17/30], Training loss: 37.1630 ; Validation loss: 4.4112\n",
      "Epoch [18/30], Training loss: 37.1481 ; Validation loss: 4.4102\n",
      "Epoch [19/30], Training loss: 37.1073 ; Validation loss: 4.4083\n",
      "Epoch [20/30], Training loss: 37.0911 ; Validation loss: 4.4062\n",
      "Epoch [21/30], Training loss: 37.0903 ; Validation loss: 4.4051\n",
      "Epoch [22/30], Training loss: 37.0880 ; Validation loss: 4.4031\n",
      "Epoch [23/30], Training loss: 37.0619 ; Validation loss: 4.4023\n",
      "Epoch [24/30], Training loss: 37.0447 ; Validation loss: 4.4021\n",
      "Epoch [25/30], Training loss: 37.0107 ; Validation loss: 4.4016\n",
      "Epoch [26/30], Training loss: 37.0399 ; Validation loss: 4.3994\n",
      "Epoch [27/30], Training loss: 37.0208 ; Validation loss: 4.3992\n",
      "Epoch [28/30], Training loss: 37.0026 ; Validation loss: 4.3995\n",
      "Epoch [29/30], Training loss: 37.0128 ; Validation loss: 4.3978\n",
      "Epoch [30/30], Training loss: 37.0233 ; Validation loss: 4.3981\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "# Train the model\n",
    "model_opt, loss_train, loss_val = training_val_seq2seq(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate)\n",
    "torch.save(model_opt.state_dict(), 'model_attention.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training/validation loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(torch.arange(num_epochs)+1, loss_train)\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(torch.arange(num_epochs)+1, loss_val)\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Validation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  6.700839996337891\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss_test = evaluate_seq2seq(model, test_dataloader, loss_fn)\n",
    "print('Test loss: ', loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q3**</span> Put the training/validation loss plot in your report.\n",
    "\n",
    "## Perspectives\n",
    "\n",
    "Let's quickly check the results in terms of machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: ['eine', 'menschenmenge', 'steht', 'auf', 'einem', '<unk>', ',', 'whrend', 'ein', 'mann', 'fotografiert', '.']\n",
      "Translation - true: ['a', 'crowd', 'of', 'people', 'are', 'standing', 'together', 'on', 'a', 'sidewalk', ',', 'while', 'one', 'man', 'is', 'taking', 'a', 'picture', '.']\n",
      "Translation - predicted: ['each', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy']\n"
     ]
    }
   ],
   "source": [
    "# Get some examples (source and target) in the test set\n",
    "example_batch_src, example_batch_trg = example_batch.src, example_batch.trg\n",
    "\n",
    "# Compute predictions with the model\n",
    "example_batch_trg_pred, _ = model(example_batch_src, len(example_batch_trg))\n",
    "indx_pred = torch.argmax(example_batch_trg_pred, -1)\n",
    "\n",
    "# Compute the source, target, and predicted sentence\n",
    "indx_sentence_print = 1\n",
    "sentence_src = indx2tokens_list(example_batch_src[:, indx_sentence_print], SRC)\n",
    "sentence_trg = indx2tokens_list(example_batch_trg[:, indx_sentence_print], TRG)\n",
    "sentence_pred = indx2tokens_list(indx_pred[:, indx_sentence_print], TRG)\n",
    "\n",
    "# Print these sentences\n",
    "print('Source sentence:', sentence_src)\n",
    "print('Translation - true:', sentence_trg)\n",
    "print('Translation - predicted:', sentence_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on the example above, the results are not satisfactory. This might be explained by a variety of factors: we only consider a subset of the data; we used a very light network; we didn't conduct training through convergence; we learned token embeddings on-the-fly using one basic layer, etc. However, even if we address these issues specifically, performance will still be limited since we used a rather basic network structure and training protocol.\n",
    "\n",
    "There are many other strategies to improve performance for such a task (but also for other text processing / NLP tasks based on RNNs - or not):\n",
    "\n",
    "- using *bi-directionnal* recurrent networks to better account for the whole context of the sentence.\n",
    "- *skip-filtering*, which means feeding each RNN cell with the whole context vector instead of just the previous hidden state (done [here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb)).\n",
    "- *teacher forcing* in the decoder at training, which means using the ground truth token as input to each decoder cell instead of the predicted token from the previous cell (done [here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)).\n",
    "- *packed padded sentences* with masking, which allows to skip the `<pad>` token in the encoder and save time (done [here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb)).\n",
    "- *self-attention*, a key component in [transformers](https://arxiv.org/pdf/1706.03762.pdf), which are state-of-the-art architectures for machine translation (see a simplified version implemented [here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb)).\n",
    "\n",
    "And many more - research is ongoing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e4653ef3f08bbcc46c48a23d8290ce23ff8798f7d85b3da9ffbf3074ca96bd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
